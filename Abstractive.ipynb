{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Single Bilayer LSTM",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fi64aA0FFxcS",
        "outputId": "f62d8eb5-556f-4711-b1f9-f1002d4966d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        }
      },
      "source": [
        "# !pip install tensorflow==2\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
        "            if verbose:\n",
        "                print('wa.s>',W_a_dot_s.shape)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>',U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        def create_inital_state(inputs, hidden_size):\n",
        "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
        "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
        "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
        "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
        "            return fake_state\n",
        "\n",
        "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
        "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JUValOzcHtEK"
      },
      "source": [
        "#Import the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab_type": "code",
        "id": "_Jpu8qLEFxcY",
        "outputId": "4893bcd7-28f3-488a-9165-0fc659feac10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import warnings\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UVakjZ3oICgx"
      },
      "source": [
        "#Read the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wnK5o4Z1Fxcj",
        "outputId": "1c85f92b-2222-42c2-9308-aca7c3b0c3c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "data=pd.read_csv(\"/content/gdrive/My Drive/amazon-fine-food-reviews/Reviews.csv\",nrows=100000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kGNQKvCaISIn"
      },
      "source": [
        "# Drop Duplicates and NA values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cjul88oOFxcr",
        "colab": {}
      },
      "source": [
        "data.drop_duplicates(subset=['Text'],inplace=True)#dropping duplicates\n",
        "data.dropna(axis=0,inplace=True)#dropping na"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XyWhmuEd1-bf",
        "colab": {}
      },
      "source": [
        "#data.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r0xLYACiFxdJ"
      },
      "source": [
        "#Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0s6IY-x2FxdL",
        "colab": {}
      },
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XZr-u3OEFxdT",
        "outputId": "915c09b3-dc71-4b66-f0f5-ca0a19363d01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import nltk \n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def text_cleaner(text,num):\n",
        "    newString = text.lower()\n",
        "    newString = BeautifulSoup(newString, \"html.parser\").text\n",
        "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
        "    newString = re.sub('\"','', newString)\n",
        "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
        "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
        "    if(num==0):\n",
        "        tokens = [w for w in newString.split() if not w in stop_words]\n",
        "    else:\n",
        "        tokens=newString.split()\n",
        "    long_words=[]\n",
        "    for i in tokens:\n",
        "        if len(i)>1:                                                 #removing short word\n",
        "            long_words.append(i)   \n",
        "    return (\" \".join(long_words)).strip()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A2QAeCHWFxdY",
        "colab": {}
      },
      "source": [
        "#call the function\n",
        "cleaned_text = []\n",
        "for t in data['Text']:\n",
        "    cleaned_text.append(text_cleaner(t,0)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GsRXocxoFxd-",
        "colab": {}
      },
      "source": [
        "#call the function\n",
        "cleaned_summary = []\n",
        "for t in data['Summary']:\n",
        "    cleaned_summary.append(text_cleaner(t,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L1zLpnqsFxey",
        "colab": {}
      },
      "source": [
        "data['cleaned_text']=cleaned_text\n",
        "data['cleaned_summary']=cleaned_summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KT_D2cLiLy77"
      },
      "source": [
        "#Drop empty rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sYK390unFxfA",
        "colab": {}
      },
      "source": [
        "data.replace('', np.nan, inplace=True)\n",
        "data.dropna(axis=0,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vm8Fk2TCL7Sp"
      },
      "source": [
        "#Understanding the distribution of the sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MdF76AHHFxgw",
        "outputId": "bf065150-5b16-4b58-dc39-5248c58722da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_word_count = []\n",
        "summary_word_count = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in data['cleaned_text']:\n",
        "      text_word_count.append(len(i.split()))\n",
        "\n",
        "for i in data['cleaned_summary']:\n",
        "      summary_word_count.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
        "\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/j9MZeJIsptJ+AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZKD5VOWqFxhC",
        "colab": {}
      },
      "source": [
        "max_text_len=100\n",
        "max_summary_len=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yY0tEJP0FxhI",
        "colab": {}
      },
      "source": [
        "cleaned_text =np.array(data['cleaned_text'])\n",
        "cleaned_summary=np.array(data['cleaned_summary'])\n",
        "\n",
        "short_text=[]\n",
        "short_summary=[]\n",
        "\n",
        "for i in range(len(cleaned_text)):\n",
        "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
        "        short_text.append(cleaned_text[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "        \n",
        "df=pd.DataFrame({'text':short_text,'summary':short_summary})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EwLUH78CFxhg",
        "colab": {}
      },
      "source": [
        "df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RakakKHcFxhl",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vq1mqyOHOtIl"
      },
      "source": [
        "\n",
        "#Text Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oRHTgX6hFxhq",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer() \n",
        "x_tokenizer.fit_on_texts(list(x_tr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RzvLwYL_PDcx"
      },
      "source": [
        "#Rarewords and its Coverage\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y8KronV2Fxhx",
        "outputId": "e42bab75-c106-46ea-d081-c481927c5eee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "thresh=4\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 64.642614023145\n",
            "Total Coverage of rare words: 1.6609879669070917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "So-J-5kzQIeO"
      },
      "source": [
        "\n",
        "* **tot_cnt** gives the size of vocabulary (which means every unique words in the text)\n",
        " \n",
        "*   **cnt** gives me the no. of rare words whose count falls below threshold\n",
        "\n",
        "*  **tot_cnt - cnt** gives me the top most common words \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J2giEsF3Fxh3",
        "colab": {}
      },
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
        "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "#padding zero upto maximum length\n",
        "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
        "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
        "\n",
        "#size of vocabulary ( +1 for padding token)\n",
        "x_voc   =  x_tokenizer.num_words + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DCbGMsm4FxiA",
        "outputId": "2736309f-ce37-4e10-a50c-d28bfeb3551f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_voc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15583"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eRHqyBkBFxiJ",
        "colab": {}
      },
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "y_tokenizer = Tokenizer()   \n",
        "y_tokenizer.fit_on_texts(list(y_tr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yzE5OiRLFxiM",
        "outputId": "e4e2d659-4602-4028-8629-eccc6b205769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "thresh=6\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in y_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 75.8816749903957\n",
            "Total Coverage of rare words: 3.9278064091959872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-fswLvIgFxiR",
        "colab": {}
      },
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
        "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
        "\n",
        "#padding zero upto maximum length\n",
        "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
        "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
        "\n",
        "#size of vocabulary\n",
        "y_voc  =   y_tokenizer.num_words +1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pR8IX9FRFxiY",
        "outputId": "8e9a5c34-53a7-408b-8476-a8c63b73ad9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_tokenizer.word_counts['sostok'],len(y_tr)   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(73839, 73839)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kZ-vW82sFxih",
        "colab": {}
      },
      "source": [
        "#deleting the rows that contain only START and END tokens\n",
        "ind=[]\n",
        "for i in range(len(y_tr)):\n",
        "    cnt=0\n",
        "    for j in y_tr[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_tr=np.delete(y_tr,ind, axis=0)\n",
        "x_tr=np.delete(x_tr,ind, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cx5NISuMFxik",
        "colab": {}
      },
      "source": [
        "ind=[]\n",
        "for i in range(len(y_val)):\n",
        "    cnt=0\n",
        "    for j in y_val[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_val=np.delete(y_val,ind, axis=0)\n",
        "x_val=np.delete(x_val,ind, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wOtlDcthFxip"
      },
      "source": [
        "# Model building\n",
        "\n",
        "1 Bilayered LSTM for the encoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zXef38nBFxir",
        "outputId": "fe112359-7c0b-4a38-bd96-5819089a4aa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        }
      },
      "source": [
        "from keras import backend as K \n",
        "from tensorflow.keras.layers import Attention\n",
        "# K.clear_session()\n",
        "\n",
        "latent_dim = 300\n",
        "embedding_dim=100\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_text_len,))\n",
        "\n",
        "#embedding layer\n",
        "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
        "\n",
        "#Bilayered LSTM\n",
        "encoder_lstm1 = Bidirectional(LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4))\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c= encoder_lstm1(enc_emb)\n",
        "\n",
        "\n",
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "#embedding layer\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
        "\n",
        "# Attention layer,\n",
        "attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "# Concat attention input and decoder LSTM output\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "#dense layer\n",
        "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "# Define the model \n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
            "\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 100, 100)     1558300     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   [(None, 100, 600), ( 962400      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 100)    314000      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 600)          0           bidirectional[0][1]              \n",
            "                                                                 bidirectional[0][3]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 600)          0           bidirectional[0][2]              \n",
            "                                                                 bidirectional[0][4]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 600),  1682400     embedding_1[0][0]                \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer (AttentionLayer ((None, None, 600),  720600      bidirectional[0][0]              \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concat_layer (Concatenate)      (None, None, 1200)   0           lstm_1[0][0]                     \n",
            "                                                                 attention_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 3140)   3771140     concat_layer[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 9,008,840\n",
            "Trainable params: 9,008,840\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lwfi1Fm8Fxiz",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s-A3J92MUljB",
        "colab": {}
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ETnPzA4OFxi3",
        "outputId": "086ef689-f94f-4649-dafc-5f35cf96d038",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=1,callbacks=[es],batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 72657 samples, validate on 8072 samples\n",
            "72657/72657 [==============================] - 240s 3ms/sample - loss: 2.4715 - val_loss: 2.2815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0ezKYOp2UxG5"
      },
      "source": [
        "#Understanding the Diagnostic plot\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tDTNLAURFxjE",
        "outputId": "090107ce-d4e6-4cdb-80f3-29bfd409242c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "from matplotlib import pyplot\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlFQCICKipBIAEREl9f8BVhL/eeDRLOgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sBX0zZnOFxjW",
        "colab": {}
      },
      "source": [
        "reverse_target_word_index=y_tokenizer.index_word\n",
        "reverse_source_word_index=x_tokenizer.index_word\n",
        "target_word_index=y_tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eM_nU_VvFxjq"
      },
      "source": [
        "# Inference\n",
        "\n",
        "Set up the inference for the encoder and decoder according to Bilayered LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9QkrNV-4Fxjt",
        "colab": {}
      },
      "source": [
        "# Encode the input sequence to get the feature vector\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim*2,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim*2,))\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim*2))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) \n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "#attention inference\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6f6TTFnBFxj6",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "    \n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    \n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "      \n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        \n",
        "        if(sampled_token!='eostok'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6GuDf4TPWt6_"
      },
      "source": [
        "Functions to convert an integer sequence to a word sequence for summary as well as the reviews:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aAUntznIFxj9",
        "colab": {}
      },
      "source": [
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
        "            newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znu7TWZK3maa",
        "colab_type": "text"
      },
      "source": [
        "#Evaluation\n",
        "Evaluating the trained model by calcuating the Rogue Score\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZNlkHjQA83U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "totalReviews = len(y_val)\n",
        "listOfReferences = [seq2summary(y_val[i]) for i in range(totalReviews)]\n",
        "listOfHypothesis = [decode_sequence(x_val[i].reshape(1,max_text_len)) for i in range(totalReviews)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft4rrCnoHSC6",
        "colab_type": "code",
        "outputId": "9b583335-449a-4814-fc75-f51a346ea971",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "\n",
        "!pip3 install rouge\n",
        "import rouge\n",
        "r = rouge.Rouge()\n",
        "r.get_scores(listOfHypothesis, listOfReferences, avg=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/63/ac/b93411318529980ab7f41e59ed64ec3ffed08ead32389e29eb78585dd55d/rouge-0.3.2-py3-none-any.whl\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-0.3.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge-1': {'f': 0.08685554587072734,\n",
              "  'p': 0.11004707631318157,\n",
              "  'r': 0.07995790858464302},\n",
              " 'rouge-2': {'f': 0.013784873620564278,\n",
              "  'p': 0.018417575156921044,\n",
              "  'r': 0.01271119448770589},\n",
              " 'rouge-l': {'f': 0.07782032332468504,\n",
              "  'p': 0.10887016848364736,\n",
              "  'r': 0.07944108185064579}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6KAbWfF48E8",
        "colab_type": "code",
        "outputId": "f1bcaaf8-299b-4112-da22-2bb8c29573bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(listOfReferences[0:5], listOfHypothesis[0:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['good value ', 'only wish it was made in the us ', 'my secret treat ', 'great toy for dogs ', 'canidae rocks '] [' great taste', ' great product', ' great product', ' my dog loves these', ' my dog loves these']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}