{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extractive",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV_GUpXPEVbz",
        "colab_type": "text"
      },
      "source": [
        "#Text rank#\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoakRPvREh20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import networkx as nx \n",
        "from copy import deepcopy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F0C9Qj9FJS-",
        "colab_type": "code",
        "outputId": "6ffb8a1d-8769-4ae5-aab2-0250916efd83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "# download and required modules of nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# setting up gdrive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDDkXMRyFW29",
        "colab_type": "text"
      },
      "source": [
        "#Dataset : tmp#\n",
        "Description,size\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cULA_qDF4Ks",
        "colab_type": "code",
        "outputId": "6600df25-7979-4188-d756-4d92c0fd66c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# testing phase \n",
        "# will change later to more complex \n",
        "# this cell must be well abstracted\n",
        "\n",
        "articles = {\"business\":[]}\n",
        "summaries = {\"business\":[]}\n",
        "# file_count = {\"business\":510,\"entertainment\":386,\"sport\":511,\"politics\":417,\"tech\":401}\n",
        "file_count = {\"business\":510}\n",
        "for k,v in file_count.items():\n",
        "  afile = \"gdrive/My Drive/BBC_News_Summary/News_Articles/\"+k+\"/\"\n",
        "  sfile = \"/gdrive/My Drive/BBC_News_Summary/Summaries/\"+k+\"/\"\n",
        "  for i in range(1,1+v):\n",
        "    afname = afile+(\"0\"*(3-len(str(i))))+str(i)+\".txt\"\n",
        "    sfname = afile+(\"0\"*(3-len(str(i))))+str(i)+\".txt\"\n",
        "    with open(afname) as f:\n",
        "      articles[k].append(f.read())\n",
        "    with open(sfname) as f:\n",
        "      summaries[k].append(f.read())\n",
        "  print(\"done reading \",k)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done reading  business\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RQssCD8UD8J",
        "colab_type": "code",
        "outputId": "d899dc29-3d16-4812-da4e-70bbd21f168e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "print(articles[\"business\"][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ad sales boost Time Warner profit\n",
            "\n",
            "Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\n",
            "\n",
            "The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\n",
            "\n",
            "Time Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.\n",
            "\n",
            "Time Warner's fourth quarter profits were slightly better than analysts' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.\n",
            "\n",
            "TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpodsDr1Gh47",
        "colab_type": "code",
        "outputId": "0d44ce1b-25c4-4c47-afab-358ed272f56c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# splitting into sentences\n",
        "# sentences = deepcopy(articles)\n",
        "# for k,v in sentences.items():\n",
        "#   sentences.extend(sent_tokenize(s))\n",
        "sentences = {\"business\":[]}\n",
        "for category,texts in articles.items():\n",
        "  print(\"sentence tokenizing \",category)\n",
        "  for text in texts:\n",
        "    sentences[category].append(sent_tokenize(text))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentence tokenizing  business\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBqdS5ptMpeI",
        "colab_type": "code",
        "outputId": "22644cff-7e14-42b3-d654-5938b2793f70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#print sample sentences\n",
        "print(sentences[\"business\"][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Ad sales boost Time Warner profit\\n\\nQuarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.', 'The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales.', 'TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn.', 'Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.', 'Time Warner said on Friday that it now owns 8% of search-engine Google.', 'But its own internet business, AOL, had has mixed fortunes.', 'It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters.', \"However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues.\", \"It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband.\", 'TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.', \"Time Warner's fourth quarter profits were slightly better than analysts' expectations.\", 'But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results.', 'For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn.', '\"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said.', 'For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.', 'TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators.', 'It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC.', 'The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m.', \"It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue.\", 'It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_J6LbeXNIVz",
        "colab_type": "text"
      },
      "source": [
        "#Initialising Word Embeddings(Glove)#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_MVklIGNHhm",
        "colab_type": "code",
        "outputId": "b15dbfbb-bd59-4caa-922f-3db98691123f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "# Uncomment below commands to download zip\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-24 14:36:48--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-11-24 14:36:48--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2019-11-24 14:36:49--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.98MB/s    in 6m 29s  \n",
            "\n",
            "2019-11-24 14:43:18 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zrk2OG4vN8bV",
        "colab_type": "code",
        "outputId": "0233bea1-946c-4aad-e8b4-a3f9738db0e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "word_emb = dict()\n",
        "with open('glove.6B.100d.txt',encoding='utf-8') as f:\n",
        "  for line in f :\n",
        "    tokens = line.strip().split()\n",
        "    word_emb[tokens[0]] = np.asarray(tokens[1:],dtype='float32')\n",
        "print(\"Initialised \"+str(len(word_emb))+\" embeddings\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialised 400000 embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kgvv8I7Q9ZQ",
        "colab_type": "text"
      },
      "source": [
        "#Preprocessing Stage#\n",
        "* Implement alternate methods for calculating sentence vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMRRH5ViRCYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text cleaning \n",
        "\n",
        "# remove stop_words\n",
        "stop_words = stopwords.words('english')\n",
        "def remove_stopwords(sen):\n",
        "    sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
        "    return sen_new\n",
        "\n",
        "clean_sentences = deepcopy(sentences)\n",
        "for category,texts in sentences.items():\n",
        "  for i in range(len(texts)):\n",
        "    # here texts[i] is list of sentences\n",
        "    clean_sentences[category][i] = pd.Series(texts[i]).str.replace(\"[^a-zA-Z]\",\" \")\n",
        "    clean_sentences[category][i] = [s.lower() for s in clean_sentences[category][i]]\n",
        "    clean_sentences[category][i] = [remove_stopwords(r.split()) for r in clean_sentences[category][i]]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ks4k6oLGYztM",
        "colab_type": "code",
        "outputId": "17db56e5-9635-40be-a8e4-d5d7e2af4525",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(clean_sentences[\"business\"][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ad sales boost time warner profit quarterly profits us media giant timewarner jumped bn three months december year earlier', 'firm one biggest investors google benefited sales high speed internet connections higher advert sales', 'timewarner said fourth quarter sales rose bn bn', 'profits buoyed one gains offset profit dip warner bros less users aol', 'time warner said friday owns search engine google', 'internet business aol mixed fortunes', 'lost subscribers fourth quarter profits lower preceding three quarters', 'however company said aol underlying profit exceptional items rose back stronger internet advertising revenues', 'hopes increase subscribers offering online service free timewarner internet customers try sign aol existing customers high speed broadband', 'timewarner also restate results following probe us securities exchange commission sec close concluding', 'time warner fourth quarter profits slightly better analysts expectations', 'film division saw profits slump helped box office flops alexander catwoman sharp contrast year earlier third final film lord rings trilogy boosted results', 'full year timewarner posted profit bn performance revenues grew bn', 'financial performance strong meeting exceeding full year objectives greatly enhancing flexibility chairman chief executive richard parsons said', 'timewarner projecting operating earnings growth around also expects higher revenue wider profit margins', 'timewarner restate accounts part efforts resolve inquiry aol us market regulators', 'already offered pay settle charges deal review sec', 'company said unable estimate amount needed set aside legal reserves previously set', 'intends adjust way accounts deal german music publisher bertelsmann purchase stake aol europe reported advertising revenue', 'book sale stake aol europe loss value stake']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCuFQBdBU8BK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vectorizing sentences\n",
        "# we take the average of word vectors as sentence vector\n",
        "# note that word vectors are of length 100\n",
        "zero_vec = np.zeros((100,))\n",
        "def get_sentence_vectors(text):\n",
        "  ret = []\n",
        "  for s in text:\n",
        "    if len(s) >0 :\n",
        "      words = s.split()\n",
        "      avg_vec = np.mean([word_emb.get(w,zero_vec) for w in words],0)\n",
        "      ret.append(avg_vec)\n",
        "    else:\n",
        "      ret.append(zero_vec)\n",
        "  return np.array(ret)\n",
        "\n",
        "\n",
        "sentence_vectors = deepcopy(clean_sentences)\n",
        "for category,texts in clean_sentences.items():\n",
        "  for i in range(len(texts)):\n",
        "    sentence_vectors[category][i] = get_sentence_vectors(texts[i])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2UGeyOmboa9",
        "colab_type": "code",
        "outputId": "d7385d89-0c02-4155-e3a8-48330dd8bb5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "print(sentence_vectors[\"business\"][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.24647717  0.13236724  0.17578991 ...  0.07944505  0.6691668\n",
            "  -0.13111262]\n",
            " [ 0.25836852  0.08868428  0.09233036 ... -0.20717452  0.73004615\n",
            "   0.21344498]\n",
            " [ 0.13221574  0.14556488  0.2931974  ...  0.01991749  0.57184434\n",
            "  -0.2566259 ]\n",
            " ...\n",
            " [ 0.01733747  0.11718404  0.11662699 ... -0.1930045   0.37169042\n",
            "  -0.00848834]\n",
            " [ 0.21365242 -0.07733981  0.17945156 ... -0.02334311  0.6738694\n",
            "   0.23502226]\n",
            " [ 0.34443122  0.182348    0.5066116  ... -0.11145437  0.5981372\n",
            "   0.24595124]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26hA6NJYZGrp",
        "colab_type": "text"
      },
      "source": [
        "#Similarity Matrix#\n",
        "* Implement cosine,unit overlap, and others"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkmyiZ7AZUCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_similarity_matrix(sen_vec):\n",
        "  num_sentences = sen_vec.shape[0]\n",
        "  similarity_matrix = np.zeros((num_sentences,)*2)\n",
        "  for i in range(num_sentences):\n",
        "    for j in range(num_sentences):\n",
        "      if i==j:\n",
        "        # ignore self similarity\n",
        "        continue;\n",
        "      similarity_matrix[i][j] = cosine_similarity(sen_vec[i].reshape(1,100),\n",
        "                                                  sen_vec[j].reshape(1,100))[0][0]\n",
        "  return similarity_matrix\n",
        "similarity_matrices = deepcopy(sentence_vectors)\n",
        "for category,texts in sentence_vectors.items():\n",
        "  for i in range(len(texts)):\n",
        "    # here texts[i] is a list of sentence vectors\n",
        "    similarity_matrices[category][i] = get_similarity_matrix(texts[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Riq2nU1Jc9Vm",
        "colab_type": "code",
        "outputId": "33e87a27-e492-4769-f4e4-91154e8d1151",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(similarity_matrices[\"business\"][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.89925611 0.87145078 0.89484477 0.82644361 0.79190433\n",
            "  0.88884628 0.92014909 0.7916562  0.83178121 0.91676897 0.87958807\n",
            "  0.90647203 0.79136986 0.88116074 0.81461054 0.7956745  0.81737548\n",
            "  0.87152338 0.85676879]\n",
            " [0.89925611 0.         0.74273962 0.91161221 0.83683413 0.88447785\n",
            "  0.81545109 0.94604707 0.89964169 0.78342092 0.87405753 0.81744307\n",
            "  0.80953205 0.78997701 0.88832623 0.820122   0.76927036 0.81066215\n",
            "  0.8732748  0.82911271]\n",
            " [0.87145078 0.74273962 0.         0.78734899 0.67249161 0.6240443\n",
            "  0.82458907 0.79127139 0.62673223 0.68000931 0.82386166 0.72861052\n",
            "  0.9165957  0.64636242 0.79851735 0.61898148 0.60613608 0.66212976\n",
            "  0.70523077 0.74159408]\n",
            " [0.89484477 0.91161221 0.78734899 0.         0.73874545 0.83951336\n",
            "  0.84482276 0.91905344 0.81016576 0.70736945 0.91018081 0.77999681\n",
            "  0.84090441 0.69925678 0.91256934 0.76933712 0.69506717 0.73382521\n",
            "  0.82659584 0.83360261]\n",
            " [0.82644361 0.83683413 0.67249161 0.73874545 0.         0.76745552\n",
            "  0.70770067 0.84388286 0.80221701 0.76503235 0.76838517 0.7705946\n",
            "  0.65793341 0.79090393 0.69224983 0.77005953 0.76572412 0.80633944\n",
            "  0.85375881 0.79308265]\n",
            " [0.79190433 0.88447785 0.6240443  0.83951336 0.76745552 0.\n",
            "  0.69329035 0.86228758 0.84590435 0.69627893 0.76796168 0.70851707\n",
            "  0.71280003 0.68096429 0.7558502  0.77777576 0.65887409 0.65828896\n",
            "  0.85344571 0.78870279]\n",
            " [0.88884628 0.81545109 0.82458907 0.84482276 0.70770067 0.69329035\n",
            "  0.         0.82127333 0.72508794 0.73722076 0.91655391 0.84169275\n",
            "  0.83141989 0.68862242 0.85774821 0.68953711 0.69974035 0.77524298\n",
            "  0.73257983 0.78010261]\n",
            " [0.92014909 0.94604707 0.79127139 0.91905344 0.84388286 0.86228758\n",
            "  0.82127333 0.         0.86007261 0.79230046 0.89716524 0.82487613\n",
            "  0.84346628 0.82523698 0.90486372 0.82909596 0.79470736 0.85425365\n",
            "  0.90876913 0.88036621]\n",
            " [0.7916562  0.89964169 0.62673223 0.81016576 0.80221701 0.84590435\n",
            "  0.72508794 0.86007261 0.         0.69437724 0.74974215 0.67553312\n",
            "  0.71669579 0.71697885 0.782839   0.78812063 0.76241583 0.75586516\n",
            "  0.86010706 0.78522027]\n",
            " [0.83178121 0.78342092 0.68000931 0.70736945 0.76503235 0.69627893\n",
            "  0.73722076 0.79230046 0.69437724 0.         0.77003539 0.75928181\n",
            "  0.69638091 0.80856079 0.72868091 0.89055097 0.87006956 0.81224275\n",
            "  0.77509224 0.73870105]\n",
            " [0.91676897 0.87405753 0.82386166 0.91018081 0.76838517 0.76796168\n",
            "  0.91655391 0.89716524 0.74974215 0.77003539 0.         0.87333715\n",
            "  0.82870793 0.78827769 0.90412557 0.76220512 0.73178858 0.81371206\n",
            "  0.79718161 0.8073259 ]\n",
            " [0.87958807 0.81744307 0.72861052 0.77999681 0.7705946  0.70851707\n",
            "  0.84169275 0.82487613 0.67553312 0.75928181 0.87333715 0.\n",
            "  0.7673806  0.77958196 0.76696193 0.69781423 0.72469819 0.80445367\n",
            "  0.75544077 0.75155324]\n",
            " [0.90647203 0.80953205 0.9165957  0.84090441 0.65793341 0.71280003\n",
            "  0.83141989 0.84346628 0.71669579 0.69638091 0.82870793 0.7673806\n",
            "  0.         0.69125623 0.86958563 0.67588216 0.66360933 0.6966114\n",
            "  0.76679778 0.77680773]\n",
            " [0.79136986 0.78997701 0.64636242 0.69925678 0.79090393 0.68096429\n",
            "  0.68862242 0.82523698 0.71697885 0.80856079 0.78827769 0.77958196\n",
            "  0.69125623 0.         0.75256449 0.79351628 0.77359235 0.83287793\n",
            "  0.77719671 0.73169333]\n",
            " [0.88116074 0.88832623 0.79851735 0.91256934 0.69224983 0.7558502\n",
            "  0.85774821 0.90486372 0.782839   0.72868091 0.90412557 0.76696193\n",
            "  0.86958563 0.75256449 0.         0.75415432 0.68578357 0.75959647\n",
            "  0.790326   0.79044157]\n",
            " [0.81461054 0.820122   0.61898148 0.76933712 0.77005953 0.77777576\n",
            "  0.68953711 0.82909596 0.78812063 0.89055097 0.76220512 0.69781423\n",
            "  0.67588216 0.79351628 0.75415432 0.         0.85386437 0.81659496\n",
            "  0.85399067 0.77824885]\n",
            " [0.7956745  0.76927036 0.60613608 0.69506717 0.76572412 0.65887409\n",
            "  0.69974035 0.79470736 0.76241583 0.87006956 0.73178858 0.72469819\n",
            "  0.66360933 0.77359235 0.68578357 0.85386437 0.         0.86975378\n",
            "  0.8063159  0.75936455]\n",
            " [0.81737548 0.81066215 0.66212976 0.73382521 0.80633944 0.65828896\n",
            "  0.77524298 0.85425365 0.75586516 0.81224275 0.81371206 0.80445367\n",
            "  0.6966114  0.83287793 0.75959647 0.81659496 0.86975378 0.\n",
            "  0.80947918 0.7687282 ]\n",
            " [0.87152338 0.8732748  0.70523077 0.82659584 0.85375881 0.85344571\n",
            "  0.73257983 0.90876913 0.86010706 0.77509224 0.79718161 0.75544077\n",
            "  0.76679778 0.77719671 0.790326   0.85399067 0.8063159  0.80947918\n",
            "  0.         0.9184649 ]\n",
            " [0.85676879 0.82911271 0.74159408 0.83360261 0.79308265 0.78870279\n",
            "  0.78010261 0.88036621 0.78522027 0.73870105 0.8073259  0.75155324\n",
            "  0.77680773 0.73169333 0.79044157 0.77824885 0.75936455 0.7687282\n",
            "  0.9184649  0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfravKYVcdPy",
        "colab_type": "text"
      },
      "source": [
        "#Applying Ranking Algorithm#\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQhawpU6cosx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ranked_sentences(sim_mat,sen):\n",
        "  nx_graph = nx.from_numpy_array(sim_mat)\n",
        "  scores = nx.pagerank(nx_graph)\n",
        "  r_sen = sorted(((scores[i],i,s) for i,s in enumerate(sen)), reverse=True)\n",
        "  r_sen = [(i,y) for (x,i,y) in r_sen]\n",
        "  return r_sen\n",
        "\n",
        "\n",
        "ranked_sentences = deepcopy(sentences)\n",
        "for category,texts in sentences.items():\n",
        "  for i in range(len(texts)):\n",
        "    ranked_sentences[category][i] = get_ranked_sentences(similarity_matrices[category][i],sentences[category][i])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W93z_kCne-w9",
        "colab_type": "code",
        "outputId": "bfe1a1f7-306d-4d27-ee4e-8b0d4e10066f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "print(\"-------------:predicted:-------------\")\n",
        "sen = sorted(ranked_sentences[\"business\"][1][:6])\n",
        "for i,s in sen[:6]:\n",
        "  print(s+'\\n')\n",
        "  # print in increasing order of i\n",
        "  \n",
        "print(\"\\n-------------:actual:-------------\")\n",
        "print(summaries[\"business\"][1])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------:predicted:-------------\n",
            "Dollar gains on Greenspan speech\n",
            "\n",
            "The dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\n",
            "\n",
            "On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data.\n",
            "\n",
            "\"He's taking a longer-term view, laying out a set of conditions under which the current account deficit can improve this year and next.\"\n",
            "\n",
            "In the meantime, the US Federal Reserve's decision on 2 February to boost interest rates by a quarter of a point - the sixth such move in as many months - has opened up a differential with European rates.\n",
            "\n",
            "The recent falls have partly been the result of big budget deficits, as well as the US's yawning current account gap, both of which need to be funded by the buying of US bonds and assets by foreign firms and governments.\n",
            "\n",
            "The White House will announce its budget on Monday, and many commentators believe the deficit will remain at close to half a trillion dollars.\n",
            "\n",
            "\n",
            "-------------:actual:-------------\n",
            "Dollar gains on Greenspan speech\n",
            "\n",
            "The dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\n",
            "\n",
            "And Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data. \"I think the chairman's taking a much more sanguine view on the current account deficit than he's taken for some time,\" said Robert Sinche, head of currency strategy at Bank of America in New York. \"He's taking a longer-term view, laying out a set of conditions under which the current account deficit can improve this year and next.\"\n",
            "\n",
            "Worries about the deficit concerns about China do, however, remain. China's currency remains pegged to the dollar and the US currency's sharp falls in recent months have therefore made Chinese export prices highly competitive. But calls for a shift in Beijing's policy have fallen on deaf ears, despite recent comments in a major Chinese newspaper that the \"time is ripe\" for a loosening of the peg. The G7 meeting is thought unlikely to produce any meaningful movement in Chinese policy. In the meantime, the US Federal Reserve's decision on 2 February to boost interest rates by a quarter of a point - the sixth such move in as many months - has opened up a differential with European rates. The half-point window, some believe, could be enough to keep US assets looking more attractive, and could help prop up the dollar. The recent falls have partly been the result of big budget deficits, as well as the US's yawning current account gap, both of which need to be funded by the buying of US bonds and assets by foreign firms and governments. The White House will announce its budget on Monday, and many commentators believe the deficit will remain at close to half a trillion dollars.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUzI2YylfQ8c",
        "colab_type": "text"
      },
      "source": [
        "#Results#\n",
        "* print \n",
        "* rouge and other measures to evaluate performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOXlEQdKn_B7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "listOfHypothesis = []\n",
        "listOfReferences = []\n",
        "# testing only for business\n",
        "# extract top 6 sentences\n",
        "max_top = 3\n",
        "for i in range(len(ranked_sentences[\"business\"])):\n",
        "  sen = (sorted(ranked_sentences[\"business\"][i][:max_top]))\n",
        "  sen = [s for i,s in sen]\n",
        "  sen = '\\n'.join(sen)\n",
        "  listOfHypothesis.append(sen)\n",
        "  listOfReferences.append(summaries[\"business\"][i])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX4qfzsJTyuM",
        "colab_type": "code",
        "outputId": "fbfea769-a7d6-4bda-ebbb-add7572983fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "source": [
        "print(listOfHypothesis[0])\n",
        "print(\"---------------\")\n",
        "print(listOfReferences[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ad sales boost Time Warner profit\n",
            "\n",
            "Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\n",
            "The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales.\n",
            "However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues.\n",
            "---------------\n",
            "Ad sales boost Time Warner profit\n",
            "\n",
            "Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (£600m) for the three months to December, from $639m year-earlier.\n",
            "\n",
            "The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\n",
            "\n",
            "Time Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.\n",
            "\n",
            "Time Warner's fourth quarter profits were slightly better than analysts' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.\n",
            "\n",
            "TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k82_oB9afbzA",
        "colab_type": "code",
        "outputId": "6034e3cc-9979-42bb-f185-c4dad7b54b2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "!pip3 install rouge\n",
        "import rouge\n",
        "r = rouge.Rouge()\n",
        "r.get_scores(listOfHypothesis, listOfReferences, avg=True)['rouge-1']['r']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/63/ac/b93411318529980ab7f41e59ed64ec3ffed08ead32389e29eb78585dd55d/rouge-0.3.2-py3-none-any.whl\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-0.3.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3224343493191521"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THd9r6Vwb8FE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # lsa\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# def get_term_doc_matrix(X):\n",
        "#   vectorizer = TfidfVectorizer(stop_words='english',\n",
        "#   max_df = 0.5,smooth_idf=True)\n",
        "#   doc_term = vectorizer.fit_transform(X)\n",
        "#   n,m = doc_term.shape\n",
        "#   term_doc = np.zeros((m,n))\n",
        "#   for i in range(m):\n",
        "#     # print(i,'/',m)\n",
        "#     for j in range(n):\n",
        "#       term_doc[i,j] = doc_term[(j,i)]\n",
        "#   return term_doc\n",
        "\n",
        "# td_hyp = get_term_doc_matrix(listOfHypothesis)\n",
        "# td_ref = get_term_doc_matrix(listOfReferences)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W0qyGY1c2tQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_lsa_vector(term_doc,r=1):\n",
        "#   u,s,vh = np.linalg.svd(term_doc,full_matrices=False)\n",
        "#   u_imp = u[:,:r]@np.diag(s[:r])\n",
        "#   s = np.sqrt(np.sum(u_imp**2,1))\n",
        "#   return s\n",
        " \n",
        "# def cosine(u,v):\n",
        "#   return np.sum(u*v)/(np.sum(u**2)*np.sum(v**2))\n",
        "\n",
        "# max_terms = 5000\n",
        "# u = get_lsa_vector(td_hyp[:max_terms,:])\n",
        "# v = get_lsa_vector(td_ref[:max_terms,:])\n",
        "\n",
        "# print(cosine(u,v))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHTiPRlWy7g1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os.path\n",
        "from gensim import corpora\n",
        "from gensim.models import LsiModel\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bhXT0Nj2BGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_data(doc_set):\n",
        "    \"\"\"\n",
        "    Input  : docuemnt list\n",
        "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
        "    Output : preprocessed text\n",
        "    \"\"\"\n",
        "    # initialize regex tokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    # create English stop words list\n",
        "    en_stop = set(stopwords.words('english'))\n",
        "    # Create p_stemmer of class PorterStemmer\n",
        "    p_stemmer = PorterStemmer()\n",
        "    # list for tokenized documents in loop\n",
        "    texts = []\n",
        "    # loop through document list\n",
        "    for i in doc_set:\n",
        "        # clean and tokenize document string\n",
        "        raw = i.lower()\n",
        "        tokens = tokenizer.tokenize(raw)\n",
        "        # remove stop words from tokens\n",
        "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
        "        # stem tokens\n",
        "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
        "        # add tokens to list\n",
        "        texts.append(stemmed_tokens)\n",
        "    return texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ST9Pzg_0Egh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_corpus(doc_clean):\n",
        "    \"\"\"\n",
        "    Input  : clean document\n",
        "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
        "    Output : term dictionary and Document Term Matrix\n",
        "    \"\"\"\n",
        "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
        "    dictionary = corpora.Dictionary(doc_clean)\n",
        "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "    # generate LDA model\n",
        "    return dictionary,doc_term_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hQoPWKKywhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
        "    \"\"\"\n",
        "    Input  : clean document, number of topics and number of words associated with each topic\n",
        "    Purpose: create LSA model using gensim\n",
        "    Output : return LSA model\n",
        "    \"\"\"\n",
        "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
        "    # generate LSA model\n",
        "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
        "    # print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
        "    return lsamodel.print_topics(num_topics=number_of_topics, num_words=words)[0][1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdqfFosz0HPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
        "    \"\"\"\n",
        "    Input   : dictionary : Gensim dictionary\n",
        "              corpus : Gensim corpus\n",
        "              texts : List of input texts\n",
        "              stop : Max num of topics\n",
        "    purpose : Compute c_v coherence for various number of topics\n",
        "    Output  : model_list : List of LSA topic models\n",
        "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, stop, step):\n",
        "        # generate LSA model\n",
        "        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p31b9vXm9oLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LSAError(hyp,ref):\n",
        "  number_of_topics=1\n",
        "  words =10\n",
        "  clean_text = preprocess_data([hyp])\n",
        "  key_words = create_gensim_lsa_model(clean_text,number_of_topics,words)\n",
        "  hyp_words = [word.split('*') for word in key_words.split('+')]\n",
        "  hyp_dict = {}\n",
        "  for x in hyp_words:\n",
        "    hyp_dict[x[1]]=x[0]\n",
        "\n",
        "  clean_text = preprocess_data([ref])\n",
        "  key_words = create_gensim_lsa_model(clean_text,number_of_topics,words)\n",
        "  ref_words = [word.split('*') for word in key_words.split('+')]\n",
        "  ref_dict = {}\n",
        "  for x in ref_words:\n",
        "    ref_dict[x[1]]=x[0]\n",
        "\n",
        "  MSE = 0\n",
        "  for key in hyp_dict:\n",
        "    if key in ref_dict:\n",
        "      MSE += (float(hyp_dict[key])-float(ref_dict[key]))**2\n",
        "    else:\n",
        "      MSE += (float(hyp_dict[key]))**2\n",
        "  return MSE\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui48nsq4HORC",
        "colab_type": "text"
      },
      "source": [
        "The Keywords of the two summaries (predicted and given) differ by around : 21.7 %.\n",
        "<br>\n",
        "So loosely speaking they are 78.3% similar. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrEz0_Nd0SAb",
        "colab_type": "code",
        "outputId": "fefd4680-411e-4db9-d9b5-0c26cb2a2c64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "number_of_topics=1\n",
        "words=10\n",
        "# document_list,titles=load_data(\"\",\"articles.txt\")\n",
        "clean_text=preprocess_data([listOfHypothesis[1]])\n",
        "key_words=create_gensim_lsa_model(clean_text,number_of_topics,words)\n",
        "print(key_words)\n",
        "# print([word.split('*') for word in key_words.split('+')])\n",
        "# print(clean_text)\n",
        "\n",
        "clean_text=preprocess_data([listOfReferences[1]])\n",
        "key_words=create_gensim_lsa_model(clean_text,number_of_topics,words)\n",
        "print(key_words)\n",
        "# lst = [word.split('*') for word in key_words.split('+')]\n",
        "# print(lst)\n",
        "# dictionary = {}\n",
        "# for x in lst:\n",
        "#   dictionary[x[1]]=x[0]\n",
        "# print(dictionary)\n",
        "# print(LSAError(listOfHypothesis[1],listOfReferences[1]))\n",
        "\n",
        "lsa_loss = 0\n",
        "for i in range(len(listOfHypothesis)):\n",
        "  lsa_loss += LSAError(listOfHypothesis[i],listOfReferences[i])\n",
        "print(lsa_loss/len(listOfHypothesis))\n",
        "# print(clean_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.429*\"us\" + 0.214*\"feder\" + 0.214*\"dollar\" + 0.214*\"month\" + 0.214*\"deficit\" + 0.214*\"rate\" + 0.214*\"reserv\" + 0.107*\"almost\" + 0.107*\"level\" + 0.107*\"buy\"\n",
            "0.354*\"us\" + 0.310*\"dollar\" + 0.310*\"deficit\" + 0.177*\"recent\" + 0.177*\"month\" + 0.133*\"currenc\" + 0.133*\"feder\" + 0.133*\"chines\" + 0.133*\"greenspan\" + 0.133*\"reserv\"\n",
            "0.21394330196078434\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}